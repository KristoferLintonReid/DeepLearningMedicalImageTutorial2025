{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Exercise: Pick and Implement Your Favorite Model for Classification\n",
        "Instructions:\n",
        "\n",
        "In this exercise, you are required to pick one of the strategies covered in the previous notebook or create your own implementation. The strategies covered include:\n",
        "\n",
        "    Radiomics\n",
        "\n",
        "    SimpleCNN\n",
        "\n",
        "    Contrastive Models\n",
        "\n",
        "You are also free to choose one of these models or design your own approach (e.g. ResNet/VITS).\n",
        "\n",
        "Please send your completed code and answers to questions as an Excercise.ipynb file to kl2418@ic.ac.uk by end of the week.\n"
      ],
      "metadata": {
        "id": "e5UMRGF0GKsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Implement the Model\n",
        "\n",
        "    Pick a model: Select one of the approaches covered in previous exercises (SimpleCNN, Radiomics, or Contrastive models). Alternatively, if you have a different model in mind, feel free to implement it.\n",
        "\n",
        "    Write the code: Implement the chosen model and ensure it is trained on the provided dataset. You can also use models from the previous notebook.\n",
        "\n",
        "    Hyperparameter tuning: Experiment with hyperparameters such as learning rate, batch size, number of layers/estimetors the number of epochs, and loss functions. Try to optimize the model's performance.\n",
        "\n",
        "Task 2: Evaluate Model Performance\n",
        "\n",
        "    Evaluate the model: After training, evaluate the model on the test set and calculate the following metrics:\n",
        "\n",
        "        ROC AUC Score\n",
        "\n",
        "        Precision\n",
        "\n",
        "        Recall\n",
        "\n",
        "        F1 Score\n",
        "\n",
        "    Visualize the results: You may use tools like confusion matrices, accuracy plots, or any other relevant visualization to present your results."
      ],
      "metadata": {
        "id": "_8rlEUUEGjUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions for Evaluation\n",
        "\n",
        "You will be graded on both the technical aspects of the model implementation and the depth of understanding demonstrated in your answers to the following questions:\n",
        "Model Choice\n",
        "\n",
        "\n",
        "    What are the reasons you chose this model over the others? What features of the model made it appealing to you?\n",
        "\n",
        "    What parameters do you believe are most important for this model?\n",
        "\n",
        "\n",
        "    Did you encounter any challenges while implementing or training the model? How did you address them?\n",
        "\n",
        "Model Performance\n",
        "\n",
        "    What was the model's performance?\n",
        "\n",
        "        (results of your model in terms of the metrics (ROC AUC, Precision, Recall, F1 Score))\n",
        "\n",
        "    Can you explain what metric(s) you think are most relevant/important for this classification task?\n",
        "\n",
        "    How did you handle overfitting or underfitting during training?\n",
        "\n",
        "        (What techniques did you employ to mitigate overfitting or underfitting issues, if any)\n",
        "\n",
        "    What role do hyperparameters play in your model's performance?\n",
        "\n",
        "        (How did you choose hyperparameters ? Did you perform any hyperparameter optimization)\n",
        "\n",
        "    What is the impact of data augmentation on your model's performance?\n",
        "\n",
        "        (How did you implement data augmentation, and did it help improve the model's generalization)\n",
        "\n",
        "    How did you ensure that your model generalizes well to unseen data?\n",
        "\n",
        "        (What steps did you take to ensure your model does not memorize the training set and instead generalizes to the test set)\n",
        "\n",
        "Model Comparison\n",
        "\n",
        "    What are the main differences between Radiomics, SimpleCNN, and Contrastive Models?\n",
        "\n",
        "    Which approach would you choose for a real-world application and why?\n",
        "\n",
        "    What are the advantages and disadvantages of your chosen model?\n",
        "\n",
        "    Does it outperform other models in terms of speed, accuracy, or interpretability? What are its limitations?\n",
        "\n",
        "    Would you consider augmenting the model with additional techniques?\n",
        "\n",
        "    For example, would you consider adding transfer learning, batch normalization, dropout, or any other techniques to improve the modelâ€™s performance?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UVCQpkiCHFTM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6l0sAEBuG_x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pEaKqMWrG_3x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}